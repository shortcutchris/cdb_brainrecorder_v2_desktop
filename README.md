# Audio Sessions - Desktop App

Eine Desktop-Anwendung zum Aufzeichnen, Verwalten und Wiedergeben von Audio-Sessions.

## Features

- **Audio-Aufnahme** mit Live-Pegelanzeige und Laufzeit-Anzeige
- **Audio-Wiedergabe** mit Play/Pause/Stop und Fortschrittsbalken
- **CRUD-Verwaltung** f√ºr Sessions (Create, Read, Update, Delete)
- **Ger√§teauswahl** zwischen allen verf√ºgbaren Mikrofonen
- **Suchfunktion** nach Titel und Notizen
- **CSV-Export** der Session-Liste
- **Datei √∂ffnen** im Standard-Audio-Player
- **AI-Integration mit OpenAI**
  - Automatische Transkription mit gpt-4o-transcribe
  - Text-Transformation mit GPT-5 (Zusammenfassen, √úbersetzen, Strukturieren)
  - Hintergrund-Verarbeitung ohne UI-Blockierung

## OpenAI Integration

Die App unterst√ºtzt automatische Transkription und AI-gest√ºtzte Textverarbeitung.

### Setup

1. **API-Key konfigurieren**: √ñffne die Einstellungen (‚öôÔ∏è in der Toolbar) und trage deinen OpenAI API-Key ein
2. Der API-Key wird sicher in `~/.config/audio_sessions/settings.json` gespeichert

### Transkription

Die Transkription verwendet das Modell **gpt-4o-transcribe** (Nachfolger von whisper-1):

```python
# Modell: gpt-4o-transcribe
# Response Format: json (nicht verbose_json!)
# Sprache: auto-detect oder manuell (de, en, etc.)
# Prompt: "Audio Sessions, Transkription, Notizen"
```

**Wichtig**: Das `gpt-4o-transcribe` Modell unterst√ºtzt nur `response_format="json"` oder `"text"`, nicht `"verbose_json"`. Die Response enth√§lt:
- `text`: Der transkribierte Text
- `language`: Erkannte Sprache (falls verf√ºgbar)
- `duration`: Audio-L√§nge (falls verf√ºgbar)

**Verwendung**:
1. W√§hle eine Session in der Tabelle
2. √ñffne den AI-Tab im Detail-Bereich
3. Klicke auf "Transkribieren"
4. Die Transkription l√§uft im Hintergrund (Worker-Thread)
5. Status wird in der Tabelle angezeigt (üîÑ laufend, ‚úì fertig)

### Text-Transformation mit GPT-5

Die Text-Transformation verwendet **GPT-5** mit dem Chat Completions API:

```python
# Modell: gpt-5
# API: Chat Completions (client.chat.completions.create)
# Parameter: reasoning_effort (minimal, low, medium, high)
# Parameter: verbosity (low, medium, high) - wird im Prompt verarbeitet
```

**Wichtig**:
- GPT-5 verwendet die **Chat Completions API**, nicht die Responses API
- Der Parameter `reasoning_effort` steuert die Denktiefe des Modells
- `max_completion_tokens` statt `max_tokens` verwenden
- Response-Struktur: `response.choices[0].message.content`

**Verf√ºgbare Transformationen**:
1. **Zusammenfassen**: Erstellt eine pr√§gnante Zusammenfassung
2. **√úbersetzen**: √úbersetzt ins Englische
3. **Strukturieren**: Gliedert den Text in √ºbersichtliche Abschnitte

**Verwendung**:
1. Transkribiere zuerst eine Session (siehe oben)
2. W√§hle eine Transformation (Zusammenfassen/√úbersetzen/Strukturieren)
3. Passe optional Reasoning Effort und Verbosity an
4. Klicke auf "Ausf√ºhren"
5. Das Ergebnis erscheint im unteren Textfeld

**Fallback**: Falls GPT-5 nicht verf√ºgbar ist (z.B. API-Fehler), wird automatisch auf GPT-4o mit √§quivalenten Parametern zur√ºckgegriffen.

### Technische Details

**Service-Architektur**:
- `services/audio_session_service.py`: OpenAI API-Aufrufe
- `services/workers.py`: Qt QThread Worker f√ºr async Verarbeitung
- Cache-System f√ºr Transkriptionen (verhindert doppelte API-Calls)

**Python 3.9 Kompatibilit√§t**:
```python
# Statt: dict | None (Python 3.10+)
# Verwenden: Optional[dict] (Python 3.9)
from typing import Optional, Dict, Any
```

**GPT-5 vs GPT-4o**:
| Feature | GPT-5 | GPT-4o (Fallback) |
|---------|-------|-------------------|
| API | Chat Completions | Chat Completions |
| Reasoning | `reasoning_effort` Parameter | Gemappt auf `temperature` |
| Verbosity | Im Prompt | Im Prompt |
| Max Tokens | `max_completion_tokens` | `max_tokens` |

**Fehlerbehandlung**:
- Netzwerkfehler werden abgefangen und im UI angezeigt
- API-Fehler zeigen Details in der Terminal-Ausgabe
- Transkriptionsstatus wird in der Datenbank persistiert

## Technologie

- **Python 3.9+** (getestet mit Python 3.9.6)
- **PySide6 (Qt)** f√ºr die GUI
- **sounddevice + soundfile** f√ºr Audio I/O
- **SQLite** f√ºr die Datenbank
- **OpenAI SDK** (>=1.57.0) f√ºr AI-Features
  - gpt-4o-transcribe f√ºr Transkription
  - GPT-5 f√ºr Text-Transformation

## Installation

1. Erstelle ein virtuelles Environment (empfohlen):
```bash
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# oder
venv\Scripts\activate  # Windows
```

2. Installiere die Dependencies:
```bash
pip install -r requirements.txt
```

## Verwendung

Starte die App mit:
```bash
# Mit virtuellem Environment (empfohlen)
./venv/bin/python3 app.py

# Oder wenn venv aktiviert ist
python app.py
```

**Wichtig**: Stelle sicher, dass die OpenAI SDK im virtuellen Environment installiert ist. Falls `ModuleNotFoundError: No module named 'openai'` erscheint:
```bash
./venv/bin/pip install "openai>=1.57.0"
```

### Aufnahme

1. W√§hle dein Mikrofon aus dem Dropdown
2. Klicke auf "Aufnahme starten"
3. Beobachte den Live-Pegel und die Laufzeit
4. Klicke auf "Aufnahme stoppen"
5. Die Session wird automatisch in der Datenbank gespeichert

### Session-Verwaltung

- **Suchen**: Nutze das Suchfeld in der Toolbar
- **Ausw√§hlen**: Klicke auf eine Session in der Tabelle
- **Bearbeiten**: √Ñndere Titel und Notizen im Formular rechts
- **Speichern**: Klicke auf "Speichern"
- **L√∂schen**: W√§hle eine Session und klicke auf "L√∂schen" in der Toolbar
- **CSV-Export**: Klicke auf "CSV Export" in der Toolbar
- **Datei √∂ffnen**: W√§hle eine Session und klicke auf "Datei √∂ffnen"

## Projektstruktur

```
audio_sessions/
‚îú‚îÄ‚îÄ app.py                          # Main-Einstiegspunkt
‚îú‚îÄ‚îÄ recorder.py                     # Audio-Recorder Klasse
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îú‚îÄ‚îÄ main_window.py              # Hauptfenster
‚îÇ   ‚îú‚îÄ‚îÄ session_form.py             # Session-Formular
‚îÇ   ‚îú‚îÄ‚îÄ table_widget.py             # Sessions-Tabelle
‚îÇ   ‚îú‚îÄ‚îÄ ai_view.py                  # AI-Integration UI
‚îÇ   ‚îú‚îÄ‚îÄ settings_dialog.py          # Einstellungen-Dialog
‚îÇ   ‚îî‚îÄ‚îÄ player_widget.py            # Audio-Player
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ audio_session_service.py    # OpenAI API Service
‚îÇ   ‚îú‚îÄ‚îÄ workers.py                  # Qt Worker f√ºr async AI-Tasks
‚îÇ   ‚îî‚îÄ‚îÄ settings_manager.py         # Settings Management
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ repo.py                     # SQLite Repository
‚îÇ   ‚îî‚îÄ‚îÄ sessions.db                 # Datenbank (wird automatisch erstellt)
‚îú‚îÄ‚îÄ recordings/                     # Aufnahmen (wird automatisch erstellt)
‚îú‚îÄ‚îÄ translations/                   # i18n √úbersetzungen
‚îî‚îÄ‚îÄ requirements.txt                # Dependencies
```

## macOS Hinweis

Bei macOS muss die App Mikrofonberechtigungen erhalten. Beim ersten Start wird ein Dialog angezeigt. Falls die Berechtigung verweigert wurde, kann sie in den Systemeinstellungen unter "Sicherheit & Datenschutz" ‚Üí "Mikrofon" aktiviert werden.

## Packaging / Distribution

Die App kann als eigenst√§ndige Anwendung gebaut werden:

```bash
# App bauen
pyinstaller AudioSessions.spec --clean

# Gebaute App √∂ffnen
open dist/AudioSessions.app
```

Die gebaute `.app` kann in den `/Applications` Ordner verschoben und ohne Python-Installation verwendet werden.

Detaillierte Anleitung: siehe [BUILD.md](BUILD.md)

## Troubleshooting

### OpenAI API Fehler

**Problem**: `ModuleNotFoundError: No module named 'openai'`
```bash
# L√∂sung: OpenAI SDK im virtuellen Environment installieren
./venv/bin/pip install "openai>=1.57.0"

# App mit venv starten
./venv/bin/python3 app.py
```

**Problem**: `TypeError: unsupported operand type(s) for |: 'type' and 'NoneType'`
- **Ursache**: Python 3.9 unterst√ºtzt keine Union Types mit `|` Operator
- **L√∂sung**: Code verwendet jetzt `Optional[dict]` statt `dict | None`

**Problem**: `response_format 'verbose_json' is not compatible with model 'gpt-4o-transcribe'`
- **Ursache**: Das neue gpt-4o-transcribe Modell unterst√ºtzt kein `verbose_json` Format
- **L√∂sung**: Code verwendet jetzt `response_format="json"`

**Problem**: `GPT-5 nicht verf√ºgbar, Fallback auf GPT-4o`
- **Ursache**: Fr√ºhere Version versuchte die Responses API zu nutzen
- **L√∂sung**: Code verwendet jetzt Chat Completions API f√ºr GPT-5
- **Parameter**: `reasoning_effort`, `max_completion_tokens`
- **Response**: `response.choices[0].message.content`

**Problem**: `'NoneType' object is not subscriptable` bei Transformation
- **Ursache**: Fehlerhafte Zugriffe auf Responses API Struktur
- **L√∂sung**: Umstellung auf Chat Completions API mit korrekter Response-Struktur

### API-Key Konfiguration

Falls die AI-Features nicht funktionieren:
1. √ñffne Einstellungen (‚öôÔ∏è)
2. Trage deinen OpenAI API-Key ein
3. Speichere und starte die App neu
4. Pr√ºfe die Terminal-Ausgabe auf Fehlermeldungen

## Lizenz

MIT
